# ğŸ® A/B Testing for User Engagement Analysis in Mobile Games

![Domain](https://img.shields.io/badge/Domain-Data%20Science%20%7C%20Analytics-blue)
![Tech](https://img.shields.io/badge/Tools-Python%20%7C%20Pandas%20%7C%20SciPy-orange)
![Stats](https://img.shields.io/badge/Methods-A%2FB%20Testing%20%7C%20Hypothesis%20Testing-success)
![Visualization](https://img.shields.io/badge/Visualization-Matplotlib%20%7C%20Seaborn-purple)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen)

---

## ğŸ“Œ Project Overview

This project focuses on **data-driven decision making** through **A/B Testing** in the context of mobile gaming analytics.  
A synthetic experiment dataset was designed to evaluate the impact of a newly introduced game feature on **user engagement and monetization metrics**.

The analysis simulates real-world gameplay behavior by comparing a **control group** against a **test group**, measuring changes in session duration, user retention, and in-app purchases using **statistical hypothesis testing**.

---

## ğŸ¯ Objective

To apply core **data science, statistics, and experimentation principles** by:

- Evaluating group-wise user engagement performance
- Visualizing behavioral patterns across cohorts
- Performing rigorous statistical hypothesis tests
- Deriving actionable product insights from data

---

## ğŸ§ª Experimental Design & Dataset

A synthetic dataset was generated to emulate realistic mobile game user behavior with the following attributes:

- `user_id` â€“ Unique player identifier  
- `group` â€“ Experiment group (`control` / `test`)  
- `session_duration` â€“ Average gameplay session duration (minutes)  
- `retention` â€“ Binary indicator (1 = returned user, 0 = churned)  
- `in_app_purchases` â€“ Total in-game spending (USD)  

---

## ğŸ“Š Statistical Analysis Pipeline

### ğŸ” Descriptive & Exploratory Analysis
- Summary statistics to compare central tendencies and dispersion
- Distribution analysis of session duration and spending behavior

### ğŸ“ˆ Data Visualization
- **Boxplots** for session duration comparison
- **Bar charts** for retention and purchase rate analysis
- Visual validation of experiment assumptions

### ğŸ§  Hypothesis Testing
- **Independent T-Test** â€“ Mean session duration comparison  
- **Chi-Square Test** â€“ Retention rate significance  
- **Mannâ€“Whitney U Test** â€“ Non-parametric spending comparison  

---

### ğŸ“ Statistical Equations & Methodology

This project applies multiple statistical hypothesis tests to validate experimental results and ensure reliable, data-driven conclusions.


---

#### ğŸ”¹ Independent Two-Sample T-Test (Session Duration)

-Used to compare the mean session duration between the control and test groups.

#### Null Hypothesis (Hâ‚€):
Mean session duration is equal for both groups.

#### Alternative Hypothesis (Hâ‚):
Mean session duration differs between the groups.

## Test Statistic:

t = (XÌ„â‚ âˆ’ XÌ„â‚‚) / âˆš( (sâ‚Â² / nâ‚) + (sâ‚‚Â² / nâ‚‚) )


Where:

-XÌ„â‚ , XÌ„â‚‚ = sample means

-sâ‚Â² , sâ‚‚Â² = sample variances

-nâ‚ , nâ‚‚ = sample sizes

A statistically significant p-value (< 0.05) indicates a meaningful difference in session duration.


---

ğŸ”¹ Chi-Square Test of Independence (User Retention)

Used to determine whether user retention is dependent on group assignment.

Null Hypothesis (Hâ‚€):
Retention is independent of group membership.

Alternative Hypothesis (Hâ‚):
Retention depends on whether the user is in the control or test group.

Test Statistic:

Ï‡Â² = Î£ ( (O âˆ’ E)Â² / E )


Where:

O = observed frequency

E = expected frequency

A significant chi-square value confirms an association between the feature and retention behavior.

ğŸ”¹ Mannâ€“Whitney U Test (In-App Purchases)

A non-parametric test used due to skewed spending distributions.

Null Hypothesis (Hâ‚€):
Both groups originate from the same distribution.

Alternative Hypothesis (Hâ‚):
The spending distributions differ between groups.

Test Statistic:

U = nâ‚nâ‚‚ + (nâ‚(nâ‚ + 1))/2 âˆ’ Râ‚


Where:

nâ‚ = sample size of group 1

Râ‚ = rank sum of group 1

This ensures robustness when normality assumptions are violated.

ğŸ”¹ Significance Level

All statistical tests were evaluated at a 95% confidence level:

Î± = 0.05


p < Î± â†’ Reject the null hypothesis

p â‰¥ Î± â†’ Fail to reject the null hypothesis

ğŸ”¹ Assumption Validation

Before hypothesis testing:

-Distribution symmetry and outliers were inspected using boxplots

-Sample independence was ensured by experimental design

-Non-parametric tests were applied when normality was not satisfied

### ğŸ’¡ Why This Matters

-Applying multiple hypothesis tests improves:

-Statistical rigor

-Reliability of experimental conclusions

-Real-world alignment with industry A/B testing practices


---


## ğŸ“ˆ Results Summary

- **Session Duration:** Statistically significant increase in the test group  
- **Retention Rate:** Improved user retention with strong statistical confidence  
- **In-App Purchases:** Higher average spending observed in the test group  

---

## âœ… Conclusion

The experimental results indicate that the newly introduced game feature **positively impacts user engagement and monetization**.  
Findings support **broader feature rollout**, with recommendations for continued experimentation and long-term monitoring.

---

## ğŸ› ï¸ Tools & Technologies

- **Programming Language:** Python  
- **Data Analysis:** Pandas, NumPy  
- **Statistical Testing:** SciPy  
- **Visualization:** Matplotlib, Seaborn  
- **Environment:** Jupyter Notebook  
- **Deployment (Planned):** Streamlit  

---

## ğŸš€ Deployment Plan

- Interactive dashboard using **Streamlit**
- Live visualization of metrics and statistical outcomes
- Deployment via **Streamlit Cloud** or **Render**

---

## ğŸ“‚ Repository Structure


```
Data_Driven_Decisions/
â”‚
â”œâ”€â”€ ab_testing_game_analysis.py # Core analysis & statistical testing
â”œâ”€â”€ synthetic_ab_test_dataset.csv # Synthetic experiment dataset
â”œâ”€â”€ streamlit_app.py # Interactive dashboard (planned)
â””â”€â”€ README.md # Project documentation

```


---

## ğŸ“š Learning Outcomes

- Hands-on experience with **A/B Testing and experimental design**
- Practical application of **statistical hypothesis testing**
- Strong understanding of **user engagement analytics**
- Data visualization for **decision-making insights**
- End-to-end analytics workflow from data to product recommendation

---

## ğŸ‘¤ Author


**Priyanshu Aggarwal**  
Electronics & Communication Engineering  

ğŸ“§ Email: Priyanshuaggarwal.in@gmail.com  
ğŸ”— LinkedIn: https://linkedin.com/in/priyanshu1201  
ğŸ’» GitHub: https://github.com/AggarwalPriyanshu  

---

â­ If you find this repository useful, feel free to star it!

